{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 idna-3.10 requests-2.32.3 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def create_response(self, input):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class ResponseGeneratorInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def generate_response(self, input, llm:LLMInterface):\n",
    "        pass\n",
    "\n",
    "class OllamaManagerInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def list_models(self):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_model_by_index(self, index):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class ChatInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def manage_session(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_user_input(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def give_output(self,user_input):\n",
    "        pass\n",
    "\n",
    "class DatabaseInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def add(self, messages):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def delete(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conversation History class that manages the history of messages in a conversation.\n",
    "\n",
    "This class provides methods to add, retrieve, and delete the conversation history.\n",
    "\"\"\"\n",
    "class ConversationHistory(DatabaseInterface):\n",
    "    \"\"\"\n",
    "    Manages the history of messages in a conversation.\n",
    "\n",
    "    Attributes:\n",
    "        history (list): A list of message dictionaries, where each dictionary contains the keys 'role' and 'message'.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def add(self, messages):\n",
    "        \"\"\"\n",
    "        Adds a new set of messages to the conversation history.\n",
    "\n",
    "        Args:\n",
    "            messages (dict): A dictionary containing the 'role' and 'message' keys.\n",
    "        \"\"\"\n",
    "        self.history.append(messages)\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Retrieves the entire conversation history.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of message dictionaries, where each dictionary contains the 'role' and 'message' keys.\n",
    "        \"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def delete(self):\n",
    "        \"\"\"\n",
    "        Deletes the entire conversation history.\n",
    "        \"\"\"\n",
    "        self.history = []\n",
    "\n",
    "\"\"\"\n",
    "ChatLLM class that manages the interaction with a language model.\n",
    "\n",
    "This class is responsible for creating responses based on the conversation history and the input from the user.\n",
    "\"\"\"\n",
    "class ChatLLM(LLMInterface):\n",
    "    \"\"\"\n",
    "    Manages the interaction with a language model.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name or identifier of the language model to use.\n",
    "        response_generator (ResponseGeneratorInterface): An object that generates the response from the language model.\n",
    "        conversation_history (DatabaseInterface): An object that manages the conversation history.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model:str,\n",
    "        response_generator : ResponseGeneratorInterface,\n",
    "        conversation_history: DatabaseInterface\n",
    "    ):\n",
    "        self.model = model\n",
    "        self._response_generator = response_generator\n",
    "        self.history = conversation_history\n",
    "\n",
    "    def create_response(self, input:str):\n",
    "        \"\"\"\n",
    "        Creates a response to the user's input based on the conversation history.\n",
    "\n",
    "        Args:\n",
    "            input (str): The user's input.\n",
    "\n",
    "        Yields:\n",
    "            str: The generated response, chunk by chunk.\n",
    "        \"\"\"\n",
    "        total_response = \"\"\n",
    "        prompt = self._prepare_prompt(input, self.history)\n",
    "        for chunk in self._response_generator.generate_response(prompt,self):\n",
    "            total_response += chunk\n",
    "            yield chunk\n",
    "        \n",
    "        self.history.add({\"role\": 'human', 'message':input})\n",
    "        self.history.add({\"role\": 'ai', 'message':total_response})\n",
    "\n",
    "    def _prepare_prompt(self, input, llm_history):\n",
    "        \"\"\"\n",
    "        Prepares the prompt for the language model based on the conversation history.\n",
    "\n",
    "        Args:\n",
    "            input (str): The user's input.\n",
    "            llm_history (DatabaseInterface): The conversation history.\n",
    "\n",
    "        Returns:\n",
    "            str: The prepared prompt.\n",
    "        \"\"\"\n",
    "        prompt = \"\\n\".join(\n",
    "            [\n",
    "                f\"{entry['role']}: {entry['message']}\"\n",
    "                for entry in llm_history.get()\n",
    "            ]\n",
    "        )\n",
    "        prompt += f\"\\nHuman: {input}\\nAI:\"\n",
    "        return prompt\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatResponseGenerator(ResponseGeneratorInterface):\n",
    "    def __init__(self,base_url:str):\n",
    "        \"\"\"\n",
    "        Initializes the ChatResponseGenerator with the specified base URL.\n",
    "        \n",
    "        Args:\n",
    "            base_url (str): The base URL for the API endpoint.\n",
    "        \"\"\"\n",
    "        self._base_url = base_url\n",
    "\n",
    "    def generate_response(self, input, llm:LLMInterface):\n",
    "        \"\"\"\n",
    "    Generates a response from the specified language model by sending a request to the API endpoint.\n",
    "    \n",
    "    Args:\n",
    "        input (str): The input text to be processed by the language model.\n",
    "        llm (LLMInterface): The language model interface to use for generating the response.\n",
    "    \n",
    "    Yields:\n",
    "        str: The generated response, streamed in chunks.\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If there is an error communicating with the server or a connection error.\n",
    "        \"\"\"\n",
    "        _url = f\"{self._base_url}/api/generate\"\n",
    "        payload = {\"model\": llm.model, \"prompt\": input, \"stream\": True}\n",
    "        try:\n",
    "\n",
    "            response = requests.post(_url, json=payload, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = json.loads(line)\n",
    "                    if \"response\" in chunk:\n",
    "                        yield chunk[\"response\"]\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            raise RuntimeError(f\"Error communicating with server: {e}\") from e  \n",
    "        except requests.RequestException as e:\n",
    "            raise RuntimeError(f\"Connection error: {e}\") from e\n",
    "        \n",
    "\n",
    "class UserChatInterface(ChatInterface):\n",
    "    def __init__(self,ollama_manager:OllamaManagerInterface,response_generator:ResponseGeneratorInterface):\n",
    "        \"\"\"\n",
    "        Initializes the object\n",
    "        \n",
    "        Args:\n",
    "            ollama_manager (OllamaManagerInterface): The OllamaManagerInterface subclass instance to use for managing the language models.\n",
    "            response_generator (ResponseGeneratorInterface): The ResponseGeneratorInterface subclass instance to use for generating responses.\n",
    "        \"\"\"\n",
    "        self._ollama_manager = ollama_manager\n",
    "        self._response_generator = response_generator\n",
    "\n",
    "    def give_role(self,role_prompt):\n",
    "        \"\"\"\n",
    "        Assigning the role to the model by adding the role prompt to the conversation history.\n",
    "        \n",
    "        Args:\n",
    "            role_prompt (str): The prompt to be added to the conversation history as a message from the human.\n",
    "        \"\"\"\n",
    "        self._llm.history.delete()\n",
    "        self._llm.history.add({'role':'human','message':role_prompt})\n",
    "\n",
    "    def _user_input_for_model(self):\n",
    "        \"\"\"\n",
    "        Asks the user to choose a model from the available models and initializes the ChatLLM object.\n",
    "        \"\"\"\n",
    "        print(\"Available models:\")\n",
    "        self._ollama_manager.list_models()\n",
    "        while True:\n",
    "            try:\n",
    "                model_index = int(input(\"Please choose the index of the model you want to use: \"))\n",
    "                self.initialize_llm(model_index)\n",
    "                break  # Exit loop if input is valid\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "    def initialize_llm(self,model_index:int,conversation_history=ConversationHistory()):\n",
    "        self._llm = ChatLLM(self._ollama_manager.get_model_by_index(model_index)['model'],self._response_generator,conversation_history)\n",
    "    \n",
    "    def manage_session(self):\n",
    "        self._user_input_for_model()\n",
    "        while True:\n",
    "            user_input = self.get_user_input()\n",
    "            if self._should_exit(user_input):\n",
    "                break\n",
    "            self.give_output(user_input)\n",
    "\n",
    "\n",
    "    def get_user_input(self):\n",
    "        user_input = input(\"You: \")\n",
    "        print(f\"You: {user_input}\")\n",
    "        return user_input\n",
    "\n",
    "    def _should_exit(self, user_input):\n",
    "        return user_input.lower() in [\"exit\", \"quit\", \"bye\"]\n",
    "\n",
    "\n",
    "    def give_output(self, user_input):\n",
    "        print(f\"Chatbot: \", end=\"\")\n",
    "        output = self._llm.create_response(user_input)\n",
    "        for line in output:\n",
    "            print(line, end=\"\", flush=True)\n",
    "        print()\n",
    "    \n",
    "class OllamaManager(OllamaManagerInterface):\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self.available_models = []\n",
    "\n",
    "    def list_models(self):\n",
    "        if not self.available_models:\n",
    "            self._get_all_models()\n",
    "        for model in self.available_models:\n",
    "            print(model)\n",
    "\n",
    "    def get_model_by_index(self,index:int):\n",
    "        if not self.available_models:\n",
    "            self._get_all_models()\n",
    "        assert index < len(self.available_models), \"Index out of range\"\n",
    "        return self.available_models[index]\n",
    "\n",
    "    def _get_all_models(self):\n",
    "        if not self.available_models:  # Check if the cache is empty\n",
    "            url = f\"{self.base_url}/api/tags\"\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                self.available_models = response.json()[\"models\"]\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error: Could not fetch models from Ollama at {self.base_url}\")\n",
    "                print(f\"Error: Could not connect to Ollama at {self.base_url}\")\n",
    "                print(\n",
    "                    f\"Please ensure Ollama is running and accessible. Error details: {str(e)}\"\n",
    "                )\n",
    "                sys.exit(1)\n",
    "        return self.available_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_manager = OllamaManager(base_url= \"http://localhost:11434\")\n",
    "response_generator = ChatResponseGenerator(base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "{'name': 'llama3.1:latest', 'model': 'llama3.1:latest', 'modified_at': '2024-09-25T19:29:51.3062143+05:30', 'size': 4661230766, 'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8.0B', 'quantization_level': 'Q4_0'}}\n",
      "You: Hi\n",
      "Chatbot: Hello! How can I assist you today?\n",
      "You: Bye\n"
     ]
    }
   ],
   "source": [
    "user_interface = UserChatInterface(ollama_manager,response_generator)\n",
    "user_interface.manage_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "You must not take two or more actions at a particular time. The Action line must have the format: Action: action_name: argument\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "average_dog_weight:\n",
    "e.g. average_dog_weight: Collie\n",
    "returns average weight of a dog when given the breed\n",
    "\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: How much does a Bulldog weigh?\n",
    "Thought: I should look the dogs weight using average_dog_weight\n",
    "Action: average_dog_weight: Bulldog\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: A Bulldog weights 51 lbs\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: A bulldog weights 51 lbs\n",
    "\n",
    "Note: Don't give any output to this response.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLLM(ollama_manager.get_model_by_index(0)['model'],response_generator,ConversationHistory())\n",
    "llm.history.add({'role':'human','message':role_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_re = re.compile(r'^Action: (\\w+): (.*)')   # python regular expression to selection action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "def average_dog_weight(name):\n",
    "    if name in \"Scottish Terrier\": \n",
    "        return(\"Scottish Terriers average 20 lbs\")\n",
    "    elif name in \"Border Collie\":\n",
    "        return(\"a Border Collies average weight is 37 lbs\")\n",
    "    elif name in \"Toy Poodle\":\n",
    "        return(\"a toy poodles average weight is 7 lbs\")\n",
    "    else:\n",
    "        return(\"An average dog weights 50 lbs\")\n",
    "\n",
    "known_actions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"average_dog_weight\": average_dog_weight\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question,agent, max_turns=5):\n",
    "    i = 0\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = \"\"\n",
    "        for token in agent.create_response(next_prompt):\n",
    "            result += token\n",
    "            print(token,end=\"\")\n",
    "        print()\n",
    "        actions = [\n",
    "            action_re.match(a) \n",
    "            for a in result.split('\\n') \n",
    "            if action_re.match(a)\n",
    "        ]\n",
    "        if actions:\n",
    "            next_prompt = \"\"\n",
    "            for action in actions:\n",
    "                action_type, action_input = action.groups()\n",
    "                if action_type not in known_actions:\n",
    "                    raise Exception(\"Unknown action: {}: {}\".format(action_type, action_input))\n",
    "                print(\" -- running {} {}\".format(action_type, action_input))\n",
    "                observation = known_actions[action_type](action_input)\n",
    "                print(\"Observation:\", observation)\n",
    "                next_prompt += f\"Observation: {observation}\\n\"\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: To find the combined weight of the two dogs, I need to get the weights of a Border Collie and a Scottish Terrier individually and then add them together.\n",
      "\n",
      "Action: average_dog_weight: Border Collie\n",
      " -- running average_dog_weight Border Collie\n",
      "Observation: a Border Collies average weight is 37 lbs\n",
      "Thought: Now that I have one dog's weight, I need to get the weight of the other dog, the Scottish Terrier, and then add it to the first one.\n",
      "\n",
      "Action: average_dog_weight: Scottish Terrier\n",
      " -- running average_dog_weight Scottish Terrier\n",
      "Observation: Scottish Terriers average 20 lbs\n",
      "Thought: Now that I have both dogs' weights, I can simply add them together to get their combined weight. I just need to run a calculation"
     ]
    }
   ],
   "source": [
    "question = \"\"\"I have 2 dogs, a border collie and a scottish terrier. What is their combined weight\"\"\"\n",
    "query(question,llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
